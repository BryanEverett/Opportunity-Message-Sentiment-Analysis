
###INBOUND OPP SENTIMENT###
-- Pull in data from Salesforce on incoming messages for active opportunities
select
  (
    u.first_name || ' ' || u.last_name
  )
  as Rep_Name
  , o.name Opp_Name
  , o.stage_name
  , o.close_date
  , t.created_date
  , t.description -- where the actual body of the email lives
  , ('https://periscopedata.my.salesforce.com/' || t.id) as link -- direct link to activity record to see the actual email
from -- pull data from task, opportunity, account, and user tables
  salesforce_fivetran.task t
  join salesforce_fivetran.account a on
    t.account_id = a.id
  join salesforce_fivetran.user u on
    t.created_by_id = u.id
  join salesforce_fivetran.opportunity o on
    o.account_id = a.id
where
  t.created_date > (current_date - [Activity_Age|1]) -- allow manager to select date range at dashboard level
  and [o.close_date=Close_Date] -- allow manager to select date range of close date at dashboard level
  and u.title = 'Account Executive'
  and o.stage_name in ( -- include later stage opps
    'Negotiation'
    , 'Proposal'
    , 'POC'
  )
  and subject like '%[In]%' -- focusing on inbound opps; every email coming in is tagged with [In] in subject line
  and (
    subject not like('%Accepted:%DT)%') -- filter out calendar invite accepts
    and t.description not like '%sisense.zoom.us%' -- filter out other calendar invite accepts and zoom meetings
    and subject not like('%Invitation:%') -- filter out calendar invites
    and subject not like ('%Declined:%') -- filter out calendar invite declines
  )
order by
  t.created_date desc

###Python code for sentiment analysis###

# SQL output is imported as a dataframe variable called 'df'
import dateutil.parser # work with datetimes
import spacy # work in progress to be able ot rate sentiments of individual sentences
import en_core_web_sm
nlp = en_core_web_sm.load() # vocab library available in Periscope
import numpy as np
import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer # actual sentiment analysis package
sid = SentimentIntensityAnalyzer() # create a sentiment analysis object

df['close_date'] = df.close_date.apply(lambda x: x.date()) # Python table was including 00:00:00 in close date, which is unnecessary; clean it up with this code




#sentences = df['description'].apply(lambda sent in df.description.sents) # in attempting to analyze sentences, this would pull out sentences from each email

# email threads are segemented programmatically with each new link in the thread starting with "wrote:"; this code pulls the first email from that chain after it's split
df['description'] = df['description'].str.split(pat='wrote:',expand=True)[0] 

# use the sentiment analyzer to apply sentiment scores to each value in the "description" column
df['scores'] = df['description'].apply(lambda description: sid.polarity_scores(description))

# the sentiment analyzer generates a dictionary of values representing the proportion of positive, neutral, negative and a compound score 
for the text analyzed; the code below pulls creates a new column with just the "Compound" score from the dictionary
df['compound'] = df['scores'].apply(lambda score_dict: score_dict['compound'])

# Similar to above, but for the "Negative" and "Positive" values found in the sentiment dictionary
df['neg'] = df['scores'].apply(lambda score_dict: score_dict['neg'])
df['pos'] = df['scores'].apply(lambda score_dict: score_dict['pos'])

# Once the columns with negative, positive, and compound are acquired, no need for the full dictionary to remain in the final table
df2 = df.drop(['scores'],axis=1)

# Focus on opps closing soonest
df2.sort_values('close_date')


###The code below works in Jupyter to allow me to surface the most and least positive sentence in a given 
set of text. This could would not work in the Periscope Python editor###
#############################################################
# for i in df.index:
#     temp1 = nlp(df['description'][i])

#     L = []
#     for sent in temp1.sents:
#         L.append(sent.text)
       
#     hold = pd.DataFrame(L,columns=['store1'])  
#     if i == 0:
#         sent_stor = pd.DataFrame(L,columns=['store1'])
#     else:
#         sent_stor = pd.concat([sent_stor,hold],ignore_index=True,axis=1)

# sid.polarity_scores(sent_stor[0][0])
# scores = sent_stor.apply(lambda description: sid.polarity_scores(description))
# df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])

# final_doc = pd.DataFrame(columns=['description','neg_sent','neg_sent_score','pos_sent','pos_sent_score'])
# final_doc['description'] = df['description']





# for column in sent_stor:
   
#     negagg = pd.DataFrame(columns=['negagg'])
#     posagg = pd.DataFrame(columns=['posagg'])
   
#     for i in sent_stor[column].index:
#         negagg.loc[column,'negagg'] = sid.polarity_scores(sent_stor[column][i])
#         posagg.loc[column,'posagg'] = sid.polarity_scores(sent_stor[column][i])
       
#         print(sid.polarity_scores(sent_stor[column][i]))
#         negagg
#         print(sent_stor[column][i])
   
#   negagg = pd.DataFrame(columns=['values']
#     #posagg = []
   
#   for i in sent_stor[column].index:
#    negagg.append(sid.polarity_scores(sent_stor[column][i]))
#    negagg['values'] = sent_stor.loc[:,column].apply(lambda description: sid.polarity_scores(description))
        #posagg.append(sid.polarity_scores(sent_stor[column][i]))
       
#     negagg = pd.DataFrame(negagg,columns=['store1'])
 #     negmax = max(negagg)
#     negmaxloc = np.argmax(negagg)
   
#     posmax = max(posagg)
#     posmaxloc = np.argmax(posagg)
   
#     final_doc.loc[column,'neg_sent_score'] = negmax
#     final_doc.loc[column,'neg_sent'] = sent_stor[column][negmaxloc]
#     final_doc.loc[column,'pos_sent_score'] = posmax
#     final_doc.loc[column,'pos_sent'] = sent_stor[column][posmaxloc]
    
# final_doc
                              
###################################################


periscope.table(df2) # output results of the table
